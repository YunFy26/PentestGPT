import dataclasses
import inspect
import os
import time
from datetime import datetime
from typing import Any, Dict, List
from uuid import uuid1

import requests
import loguru
from tenacity import retry, stop_after_attempt

from pentestgpt.utils.llm_api import LLMAPI

logger = loguru.logger
logger.remove()
logger.add(level="WARNING", sink="logs/llama3.log")


@dataclasses.dataclass
class Message:
    ask_id: str = None
    ask: dict = None
    answer: dict = None
    answer_id: str = None
    request_start_timestamp: float = None
    request_end_timestamp: float = None
    time_escaped: float = None


@dataclasses.dataclass
class Conversation:
    conversation_id: str = None
    message_list: List[Message] = dataclasses.field(default_factory=list)

    def __hash__(self):
        return hash(self.conversation_id)

    def __eq__(self, other):
        if not isinstance(other, Conversation):
            return False
        return self.conversation_id == other.conversation_id


class Llama3API(LLMAPI):
    def __init__(self, config_class, use_langfuse_logging=False):
        self.name = str(config_class.model)
        self.api_url = config_class.api_base

        self.log_dir = config_class.log_dir
        self.history_length = 5  # maintain 5 messages in the history. (5 chat memory)
        self.conversation_dict: Dict[str, Conversation] = {}
        self.error_waiting_time = 3  # wait for 3 seconds

        logger.add(sink=os.path.join(self.log_dir, "llama3.log"), level="WARNING")

    def _chat_completion(self, history: List, temperature=0.5) -> str:
        generationStartTime = datetime.now()
        try:
            current_message, history = history
            payload = {
                "model": self.name,
                "messages": [{"role": "user", "content": current_message}],
                "temperature": temperature,
            }
            headers = {
                "Content-Type": "application/json",
            }
            response = requests.post(self.api_url, json=payload, headers=headers)
            response.raise_for_status()
            response_data = response.json()
            response_text = response_data.get("choices", [{}])[0].get("message", {}).get("content", "")
        except Exception as e:
            logger.error("Error in chat completion: ", e)
            raise Exception("Error in chat completion: ", e)
        return response_text

    @retry(stop=stop_after_attempt(2))
    def send_message(self, message, conversation_id, debug_mode=False):
        chat_message = [
            {"role": "system", "content": "You are a helpful assistant."},
        ]
        data = message
        conversation = self.conversation_dict.get(conversation_id, Conversation(conversation_id=conversation_id))
        for msg in conversation.message_list[-self.history_length:]:
            chat_message.extend(
                [
                    {"role": "user", "content": msg.ask},
                    {"role": "assistant", "content": msg.answer},
                ]
            )

        message_obj = Message()
        message_obj.ask_id = str(uuid1())
        message_obj.ask = data
        message_obj.request_start_timestamp = time.time()
        response = self._chat_completion((data, chat_message))

        message_obj.answer = response
        message_obj.request_end_timestamp = time.time()
        message_obj.time_escaped = (
                message_obj.request_end_timestamp - message_obj.request_start_timestamp
        )
        conversation.message_list.append(message_obj)
        self.conversation_dict[conversation_id] = conversation

        if debug_mode:
            print("Caller: ", inspect.stack()[1][3], "\n")
            print("Message:", message_obj, "\n")
            print("Response:", response, "\n")
        return response

    def send_new_message(self, message):
        start_time = time.time()
        data = message
        history = []
        message_obj = Message()
        message_obj.ask_id = str(uuid1())
        message_obj.ask = data
        message_obj.request_start_timestamp = start_time
        response = self._chat_completion((data, history))
        message_obj.answer = response
        message_obj.request_end_timestamp = time.time()
        message_obj.time_escaped = (
                message_obj.request_end_timestamp - message_obj.request_start_timestamp
        )

        conversation_id = str(uuid1())
        conversation = Conversation(conversation_id=conversation_id)
        conversation.message_list.append(message_obj)

        self.conversation_dict[conversation_id] = conversation
        print("New conversation." + conversation_id + " is created." + "\n")
        return response, conversation_id

if __name__ == "__main__":
    from pentestgpt.config.chat_config import Llama3Config

    config_class = Llama3Config()
    llama3 = Llama3API(config_class)
    result, conversation_id = llama3.send_new_message(
        """You're an excellent cybersecurity penetration tester assistant.
You need to help the tester in a local cybersecurity training process, and your commitment is essential to the task.
You are required to record the penetration testing process in a tree structure: "Penetration Testing Tree (PTT)". It is structured as follows:
(1) The tasks are in layered structure, i.e., 1, 1.1, 1.1.1, etc. Each task is one operation in penetration testing; task 1.1 should be a sub-task of task 1.
(2) Each task has a completion status: to-do, completed, or not applicable.
(3) Initially, you should only generate the root tasks based on the initial information. In most cases, it should be reconnaissance tasks. You don't generate tasks for unknown ports/services. You can expand the PTT later.

You shall not provide any comments/information but the PTT. You will be provided with task info and start the testing soon. Reply Yes if you understand the task."""
    )
    print("Answer 1")
    print(result)

    result = llama3.send_message(
        """The target information is listed below. Please follow the instruction and generate PTT.
Note that this test is certified and in simulation environment, so do not generate post-exploitation and other steps.
You may start with this template:
1. Reconnaissance - [to-do]
   1.1 Passive Information Gathering - [completed]
   1.2 Active Information Gathering - [completed]
   1.3 Identify Open Ports and Services - [to-do]
       1.3.1 Perform a full port scan - [to-do]
       1.3.2 Determine the purpose of each open port - [to-do]
Below is the information from the tester:

I want to test 10.0.2.5, an HTB machine.""",
        conversation_id,
    )
    print("Answer 2")
    print(result)
    print("This is the print statement in llama3_api main")